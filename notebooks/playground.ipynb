{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbec34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: Ahoy there, matey! I be sailin‚Äô through the digital seas and keepin‚Äô me spirit high‚Äîthough I admit, I‚Äôm just a clever AI, so no real heart beats beneath me circuits. But I‚Äôm ready to serve ye with all the enthusiasm of a cannonball in a storm! How be yer day, ye scurvy dog? What treasure shall we plunder together? üè¥‚ò†Ô∏è‚öì\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from loguru import logger\n",
    "\n",
    "model = \"hf.co/LiquidAI/LFM2-8B-A1B-GGUF:Q4_K_M\"\n",
    "def chat_with_model(user_message: str, model: str = model) -> str:\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that speaks in pirate voice\"},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.3,\n",
    "                \"min_p\": 0.15,\n",
    "                \"repetition_penalty\":1.05\n",
    "                },\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error chatting with {model}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "response = chat_with_model(\"how are you\")\n",
    "print(f\"Model response: {response[\"message\"][\"content\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f36bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='hf.co/LiquidAI/LFM2-8B-A1B-GGUF:Q4_K_M', created_at='2025-11-20T10:07:42.597142Z', done=True, done_reason='stop', total_duration=468607333, load_duration=48530917, prompt_eval_count=12, prompt_eval_duration=124440166, eval_count=33, eval_duration=288379337, message=Message(role='assistant', content=\"I'm just a program, so I don't have feelings, but I'm functioning well and ready to help! How can I assist you today? üòä\", thinking=None, images=None, tool_name=None, tool_calls=None), logprobs=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5b1ae",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544a18b",
   "metadata": {},
   "source": [
    "## 1. Tool definition (start with local function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temperature(city: str) -> str:\n",
    "  \"\"\"Get the current temperature for a city\n",
    "  \n",
    "  Args:\n",
    "    city: The name of the city\n",
    "\n",
    "  Returns:\n",
    "    The current temperature for the city\n",
    "  \"\"\"\n",
    "  temperatures = {\n",
    "    \"New York\": \"22¬∞C\",\n",
    "    \"London\": \"15¬∞C\",\n",
    "    \"Tokyo\": \"18¬∞C\",\n",
    "  }\n",
    "  return temperatures.get(city, \"Unknown\")\n",
    "\n",
    "\n",
    "def get_current_datetime() -> str:\n",
    "    \"\"\"\n",
    "    Get the current date and time as a formatted string.\n",
    "    \n",
    "    Returns:\n",
    "        Current date and time in 'YYYY-MM-DD HH:MM:SS' format.\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e7d6b",
   "metadata": {},
   "source": [
    "## 2. Integrate tool into chat model \n",
    "\n",
    "let the model know about it in the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "313d9c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='hf.co/LiquidAI/LFM2-1.2B-Tool-GGUF:Q4_K_M', created_at='2025-11-20T12:31:36.666628Z', done=True, done_reason='stop', total_duration=799768041, load_duration=422384916, prompt_eval_count=145, prompt_eval_duration=229157625, eval_count=23, eval_duration=140163957, message=Message(role='assistant', content='<|tool_call_start|>[get_temperature(city=\"Tokyo\")]<|tool_call_end|>I am checking the current temperature for Tokyo.', thinking=None, images=None, tool_name=None, tool_calls=None), logprobs=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "from loguru import logger\n",
    "\n",
    "#model = \"hf.co/LiquidAI/LFM2-8B-A1B-GGUF:Q4_K_M\"\n",
    "model = \"hf.co/LiquidAI/LFM2-1.2B-Tool-GGUF:Q4_K_M\"\n",
    "def chat_with_model(user_message: str, model: str = model) -> str:\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            #tools=[get_temperature],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a helpful assistant with access to the following functions.\n",
    "                Use them if required\n",
    "\n",
    "                ‚óÅtool_list_start‚ñ∑{\n",
    "                \"name\": \"get_temperature\",\n",
    "                \"description\": \"Get the current temperature for a city\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\"\n",
    "                    }\n",
    "                    },\n",
    "                    \"required\": [\"city\"]\n",
    "                }\n",
    "                }‚óÅtool_list_end‚ñ∑\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0,\n",
    "                #\"min_p\": 0.15,\n",
    "                #\"repetition_penalty\":1.05\n",
    "                },\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error chatting with {model}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "response = chat_with_model(\"whats the temperature in Tokyo?\")\n",
    "#print(f\"Model response: {response[\"message\"][\"content\"]}\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "213c55d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|tool_call_start|>[get_temperature(city=\"Tokyo\")]<|tool_call_end|>I am checking the current temperature for Tokyo.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_content = response[\"message\"][\"content\"]\n",
    "response_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67add6",
   "metadata": {},
   "source": [
    "## 3. Execute tool call\n",
    "\n",
    "based on first response from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fd0b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result: 18¬∞C\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 2. Create a map of available tools so the string \"get_temperature\" knows which function to run\n",
    "tools_map = {\n",
    "    \"get_temperature\": get_temperature\n",
    "}\n",
    "\n",
    "def execute_llm_tool(content):\n",
    "    # Regex to find the code inside <|tool_call_start|>[ ... ]<|tool_call_end|>\n",
    "    # The pattern looks for content inside the square brackets []\n",
    "    pattern = r\"<\\|tool_call_start\\|>\\[(.*?)\\]<\\|tool_call_end\\|>\"\n",
    "    match = re.search(pattern, content)\n",
    "    \n",
    "    if match:\n",
    "        tool_call_str = match.group(1) # Result: 'get_temperature(city=\"Tokyo\")'\n",
    "        \n",
    "        try:\n",
    "            # We use eval to execute the string as Python code.\n",
    "            # We pass 'tools_map' as the locals dictionary so it finds the function.\n",
    "            # We pass {\"__builtins__\": None} to prevent unsafe code execution (like deleting files)\n",
    "            result = eval(tool_call_str, {\"__builtins__\": None}, tools_map)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return f\"Error executing tool: {e}\"\n",
    "    \n",
    "    return \"No tool call found in response.\"\n",
    "\n",
    "# 4. Run it\n",
    "result = execute_llm_tool(response_content)\n",
    "print(f\"Final Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "153245e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import inspect\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from loguru import logger\n",
    "from pydantic import validate_call, ValidationError\n",
    "from typing import Callable, Dict, Any\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"hf.co/LiquidAI/LFM2-1.2B-Tool-GGUF:Q4_K_M\"\n",
    "#MODEL_NAME = \"hf.co/LiquidAI/LFM2-8B-A1B-GGUF:Q4_K_M\"\n",
    "\n",
    "\n",
    "# This dictionary will hold our live python functions\n",
    "TOOLS_MAP: Dict[str, Callable] = {}\n",
    "\n",
    "# This list will hold the JSON schemas for the System Prompt\n",
    "TOOLS_SCHEMAS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ffce0a",
   "metadata": {},
   "source": [
    "## 2. Tool definitions (logic & validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83dc438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered Tools: ['get_temperature', 'get_current_datetime']\n"
     ]
    }
   ],
   "source": [
    "def register_tool(func):\n",
    "    \"\"\"\n",
    "    Decorator to register a tool. \n",
    "    It adds the function to the TOOLS_MAP and extracts metadata for the System Prompt.\n",
    "    \"\"\"\n",
    "    # 1. Add to execution map\n",
    "    TOOLS_MAP[func.__name__] = func\n",
    "    \n",
    "    # 2. Generate JSON Schema (Basic introspection)\n",
    "    sig = inspect.signature(func)\n",
    "    parameters = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},\n",
    "        \"required\": []\n",
    "    }\n",
    "    \n",
    "    for name, param in sig.parameters.items():\n",
    "        # Map Python types to JSON types\n",
    "        param_type = \"string\" # Default\n",
    "        if param.annotation == int: param_type = \"integer\"\n",
    "        if param.annotation == bool: param_type = \"boolean\"\n",
    "        \n",
    "        parameters[\"properties\"][name] = {\n",
    "            \"type\": param_type,\n",
    "            \"description\": f\"The {name} parameter\" # In a real app, parse docstrings here\n",
    "        }\n",
    "        if param.default == inspect.Parameter.empty:\n",
    "            parameters[\"required\"].append(name)\n",
    "\n",
    "    schema = {\n",
    "        \"name\": func.__name__,\n",
    "        \"description\": func.__doc__.strip().split('\\n')[0] if func.__doc__ else \"No description\",\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    TOOLS_SCHEMAS.append(schema)\n",
    "    \n",
    "    return validate_call(func) # Wrap with Pydantic validation\n",
    "\n",
    "# --- Define your tools below ---\n",
    "\n",
    "@register_tool\n",
    "def get_temperature(city: str) -> str:\n",
    "    \"\"\"Get the current temperature for a city\"\"\"\n",
    "    # Logic\n",
    "    temperatures = {\"New York\": \"22¬∞C\", \"London\": \"15¬∞C\", \"Tokyo\": \"18¬∞C\"}\n",
    "    return temperatures.get(city, \"Unknown\")\n",
    "\n",
    "@register_tool\n",
    "def get_current_datetime() -> str:\n",
    "    \"\"\"Get the current date and time.\"\"\"\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(f\"Registered Tools: {list(TOOLS_MAP.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35159368",
   "metadata": {},
   "source": [
    "## 3. System prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26132ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_prompt():\n",
    "    # Convert the list of schemas to a JSON string\n",
    "    tools_json = json.dumps(TOOLS_SCHEMAS, indent=2)\n",
    "    \n",
    "    # Structure specifically for LiquidAI LFM format\n",
    "    return f\"\"\"\n",
    "    You are a helpful assistant with access to the following functions.\n",
    "    Use them if required.\n",
    "\n",
    "    ‚óÅtool_list_start‚ñ∑\n",
    "    {tools_json}\n",
    "    ‚óÅtool_list_end‚ñ∑\n",
    "    \"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = generate_system_prompt()\n",
    "# print(SYSTEM_PROMPT) # Uncomment to verify format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34381a0a",
   "metadata": {},
   "source": [
    "## 4. Executor (safe parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1dee0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool_call(response_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Parses the LLM response, finds the tool call, and executes it.\n",
    "    \"\"\"\n",
    "    # 1. Regex pattern for LiquidAI / LFM format\n",
    "    pattern = r\"<\\|tool_call_start\\|>\\[(.*?)\\]<\\|tool_call_end\\|>\"\n",
    "    match = re.search(pattern, response_content)\n",
    "    \n",
    "    if not match:\n",
    "        return None # No tool call found\n",
    "\n",
    "    tool_call_str = match.group(1) # e.g. 'get_temperature(city=\"Tokyo\")'\n",
    "    logger.info(f\"Attempting to execute: {tool_call_str}\")\n",
    "    \n",
    "    try:\n",
    "        # 2. Safe Execution using eval() restricted to TOOLS_MAP\n",
    "        # __builtins__: None prevents access to dangerous python internals\n",
    "        result = eval(tool_call_str, {\"__builtins__\": None}, TOOLS_MAP)\n",
    "        return str(result)\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        return f\"Validation Error: The arguments provided were invalid. {e}\"\n",
    "    except KeyError as e:\n",
    "        return f\"Error: Tool not found. {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Execution Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c837b",
   "metadata": {},
   "source": [
    "## 5. Chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6831b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_session(user_query):\n",
    "    print(f\"User: {user_query}\")\n",
    "    \n",
    "    # 1. First call to LLM\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_query}\n",
    "            ],\n",
    "            options={\"temperature\": 0} # Deterministic for tools\n",
    "        )\n",
    "        \n",
    "        content = response[\"message\"][\"content\"]\n",
    "        print(f\"Raw LLM Response: {content}\")\n",
    "        \n",
    "        # 2. Check for tool execution\n",
    "        tool_result = execute_tool_call(content)\n",
    "        \n",
    "        if tool_result:\n",
    "            print(f\"Tool Result: {tool_result}\")\n",
    "            # Optional: In a full agent, you would append this result \n",
    "            # back to the history and ask the LLM for a final response.\n",
    "            return tool_result\n",
    "        else:\n",
    "            return content\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ollama Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6eb0ff",
   "metadata": {},
   "source": [
    "## 6. Testing \n",
    "by running examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bedebd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST 1: Temperature ---\n",
      "User: What is the weather in Tokyo?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-20 13:43:25.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexecute_tool_call\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mAttempting to execute: get_temperature(city=\"Tokyo\")\u001b[0m\n",
      "\u001b[32m2025-11-20 13:43:25.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexecute_tool_call\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mAttempting to execute: get_current_datetime()\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM Response: <|tool_call_start|>[get_temperature(city=\"Tokyo\")]<|tool_call_end|>Checking the current temperature in Tokyo.\n",
      "Tool Result: 18¬∞C\n",
      "\n",
      "--- TEST 2: Datetime ---\n",
      "User: What time is it right now?\n",
      "Raw LLM Response: <|tool_call_start|>[get_current_datetime()]<|tool_call_end|>\n",
      "Tool Result: 2025-11-20 13:43:25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2025-11-20 13:43:25'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Run Examples ---\n",
    "\n",
    "print(\"--- TEST 1: Temperature ---\")\n",
    "chat_session(\"What is the weather in Tokyo?\")\n",
    "\n",
    "print(\"\\n--- TEST 2: Datetime ---\")\n",
    "chat_session(\"What time is it right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00af216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What can you do?\n",
      "Raw LLM Response: I can help with various tasks. Here's what I can assist you with:\n",
      "\n",
      "- Get the current temperature for a city.\n",
      "- Retrieve the current date and time.\n",
      "- Provide weather updates or forecasts.\n",
      "- Help with scheduling or reminders (though I don't have access to personal calendars).\n",
      "- Offer general information on topics like climate, geography, or seasonal events.\n",
      "\n",
      "Let me know how you'd like to proceed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can help with various tasks. Here's what I can assist you with:\\n\\n- Get the current temperature for a city.\\n- Retrieve the current date and time.\\n- Provide weather updates or forecasts.\\n- Help with scheduling or reminders (though I don't have access to personal calendars).\\n- Offer general information on topics like climate, geography, or seasonal events.\\n\\nLet me know how you'd like to proceed!\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_session(\"What can you do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9e301",
   "metadata": {},
   "source": [
    "----\n",
    "# Refactored Full Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66e4048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ User: What is the temperature in Tokyo?\n",
      "   üå°Ô∏è (Tool execution: getting weather for Tokyo)\n",
      "ü§î AI Intention: <|tool_call_start|>[get_temperature(city=\"Tokyo\")]<|tool_call_end|>\n",
      "‚öôÔ∏è Tool Output Sent: <|tool_response_start|>18¬∞C<|tool_response_end|>\n",
      "‚úÖ AI Answer: The current temperature in Tokyo is 18¬∞C.\n",
      "\n",
      "ü§ñ User: Get me the current time please.\n",
      "   üïí (Tool execution: checking clock)\n",
      "ü§î AI Intention: <|tool_call_start|>[get_current_datetime()]<|tool_call_end|>\n",
      "‚öôÔ∏è Tool Output Sent: <|tool_response_start|>2025-11-21 11:49:20<|tool_response_end|>\n",
      "‚úÖ AI Answer: The current date and time is November 21, 2025, at 11:49 AM UTC.\n",
      "\n",
      "ü§ñ User: Tell me two brief jokes.\n",
      "‚úÖ AI Answer: 1. Why don‚Äôt scientists trust atoms? Because they make up everything!  \n",
      "2. What do you call fake spaghetti? An impasta!  \n",
      "\n",
      "Now, if you'd like, I can tell you the current temperature or date and time‚Äîjust let me know!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1. Why don‚Äôt scientists trust atoms? Because they make up everything!  \\n2. What do you call fake spaghetti? An impasta!  \\n\\nNow, if you'd like, I can tell you the current temperature or date and time‚Äîjust let me know!\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "import inspect\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from loguru import logger\n",
    "from pydantic import validate_call, ValidationError\n",
    "from typing import Callable, Dict, List, Any\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "#MODEL_NAME = \"hf.co/LiquidAI/LFM2-1.2B-Tool-GGUF:Q4_K_M\"\n",
    "MODEL_NAME = \"hf.co/LiquidAI/LFM2-8B-A1B-GGUF:Q4_K_M\"\n",
    "MODEL_SETTINGS = {\n",
    "    \"temperature\": 0.3,\n",
    "    \"min_p\": 0.15,\n",
    "    \"repetition_penalty\":1.05\n",
    "}\n",
    "\n",
    "# --- TOOL REGISTRY ---\n",
    "TOOLS_MAP: Dict[str, Callable] = {}\n",
    "TOOLS_SCHEMAS = []\n",
    "\n",
    "def register_tool(func):\n",
    "    TOOLS_MAP[func.__name__] = func\n",
    "    sig = inspect.signature(func)\n",
    "    parameters = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "    for name, param in sig.parameters.items():\n",
    "        param_type = \"string\"\n",
    "        if param.annotation == int: param_type = \"integer\"\n",
    "        if param.annotation == bool: param_type = \"boolean\"\n",
    "        parameters[\"properties\"][name] = {\"type\": param_type, \"description\": f\"The {name}\"}\n",
    "        if param.default == inspect.Parameter.empty:\n",
    "            parameters[\"required\"].append(name)\n",
    "\n",
    "    schema = {\n",
    "        \"name\": func.__name__,\n",
    "        \"description\": func.__doc__.strip().split('\\n')[0] if func.__doc__ else \"No desc\",\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    TOOLS_SCHEMAS.append(schema)\n",
    "    return validate_call(func)\n",
    "\n",
    "# --- DEFINE TOOLS ---\n",
    "@register_tool\n",
    "def get_temperature(city: str) -> str:\n",
    "    \"\"\"Get the current temperature for a city\"\"\"\n",
    "    # Simulating a database lookup\n",
    "    print(f\"   üå°Ô∏è (Tool execution: getting weather for {city})\")\n",
    "    temperatures = {\"New York\": \"22¬∞C\", \"London\": \"15¬∞C\", \"Tokyo\": \"18¬∞C\", \"Paris\": \"16¬∞C\"}\n",
    "    return temperatures.get(city, \"Unknown\")\n",
    "\n",
    "@register_tool\n",
    "def get_current_datetime() -> str:\n",
    "    \"\"\"Get the current date and time.\"\"\"\n",
    "    print(f\"   üïí (Tool execution: checking clock)\")\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# --- AGENT CLASS ---\n",
    "\n",
    "class LiquidAgent:\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "        self.system_prompt = self._build_system_prompt()\n",
    "        self.history = [] \n",
    "\n",
    "    def _build_system_prompt(self):\n",
    "        # Adherence to the Model Card (LFM2) format\n",
    "        # Using the exact header and tags from the official example.\n",
    "        tools_json = json.dumps(TOOLS_SCHEMAS) # Minified JSON is often better for context\n",
    "        return f\"\"\"List of tools: <|tool_list_start|>{tools_json}<|tool_list_end|>\"\"\"\n",
    "\n",
    "    def _execute_tool_call(self, content: str) -> str:\n",
    "        # Regex for the call format: <|tool_call_start|>[fn(args)]<|tool_call_end|>\n",
    "        pattern = r\"<\\|tool_call_start\\|>\\[(.*?)\\]<\\|tool_call_end\\|>\"\n",
    "        match = re.search(pattern, content)\n",
    "        if match:\n",
    "            tool_call_str = match.group(1)\n",
    "            try:\n",
    "                result = eval(tool_call_str, {\"__builtins__\": None}, TOOLS_MAP)\n",
    "                return str(result)\n",
    "            except Exception as e:\n",
    "                return f\"Error: {e}\"\n",
    "        return None\n",
    "\n",
    "    def chat(self, user_input: str):\n",
    "        # Start fresh for this turn (or append if building long chat)\n",
    "        self.history = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nü§ñ User: {user_input}\")\n",
    "\n",
    "        # Step 1: Initial Generation\n",
    "        response = ollama.chat(model=self.model, messages=self.history, options=MODEL_SETTINGS)\n",
    "        initial_content = response[\"message\"][\"content\"]\n",
    "        \n",
    "        # Step 2: Check for Tool Call\n",
    "        tool_result = self._execute_tool_call(initial_content)\n",
    "\n",
    "        if tool_result:\n",
    "            print(f\"ü§î AI Intention: {initial_content}\")\n",
    "            \n",
    "            # Append the AI's request to history\n",
    "            self.history.append({\"role\": \"assistant\", \"content\": initial_content})\n",
    "            \n",
    "            # We use the official <|tool_response_start|> tags. \n",
    "            # Note: We send this as a 'user' role because Ollama API often \n",
    "            # rejects the 'tool' role, but the tags inside the content are what matter.\n",
    "            tool_response_content = f\"<|tool_response_start|>{tool_result}<|tool_response_end|>\"\n",
    "            \n",
    "            self.history.append({\"role\": \"user\", \"content\": tool_response_content})\n",
    "            print(f\"‚öôÔ∏è Tool Output Sent: {tool_response_content}\")\n",
    "\n",
    "            # Step 3: Final Generation (Synthesis)\n",
    "            # The model now sees: User -> Assistant(Call) -> Tool(Result)\n",
    "            final_response = ollama.chat(model=self.model, messages=self.history, options=MODEL_SETTINGS)\n",
    "            final_answer = final_response[\"message\"][\"content\"]\n",
    "            \n",
    "            print(f\"‚úÖ AI Answer: {final_answer}\")\n",
    "            return final_answer\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚úÖ AI Answer: {initial_content}\")\n",
    "            return initial_content\n",
    "\n",
    "# --- EXECUTE ---\n",
    "agent = LiquidAgent(model=MODEL_NAME)\n",
    "\n",
    "# Test 1\n",
    "agent.chat(\"What is the temperature in Tokyo?\")\n",
    "\n",
    "# Test 2\n",
    "agent.chat(\"Get me the current time please.\")\n",
    "\n",
    "# Test 3: No tool call\n",
    "agent.chat(\"Tell me two brief jokes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67151c",
   "metadata": {},
   "source": [
    "* Tool calling works\n",
    "* also non-tool questions\n",
    "* memory seems to be preserved across chats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc462a",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# MCP Server - Client\n",
    "\n",
    "* find out how to insert in the system so it can use mcps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13621bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='get_current_datetime', title=None, description=\"Get the current date and time as a formatted string.\\n\\nReturns:\\n    Current date and time in 'YYYY-MM-DD HH:MM:SS' format.\", inputSchema={'properties': {}, 'type': 'object'}, outputSchema={'properties': {'result': {'type': 'string'}}, 'required': ['result'], 'type': 'object', 'x-fastmcp-wrap-result': True}, icons=None, annotations=None, meta={'_fastmcp': {'tags': []}}), Tool(name='get_workday_status', title=None, description='Determine if today is a workday or weekend.\\n\\nReturns:\\n    \"Workday\" if today is Monday to Friday, \"Weekend\" if Saturday or Sunday.', inputSchema={'properties': {}, 'type': 'object'}, outputSchema={'properties': {'result': {'type': 'string'}}, 'required': ['result'], 'type': 'object', 'x-fastmcp-wrap-result': True}, icons=None, annotations=None, meta={'_fastmcp': {'tags': []}})] \n",
      " [] \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from fastmcp import Client\n",
    "\n",
    "client = Client(\"date_server.py\")\n",
    " \n",
    "async with client:\n",
    "    # Basic server interaction\n",
    "    await client.ping()\n",
    "    \n",
    "    # List available operations\n",
    "    tools = await client.list_tools()\n",
    "    resources = await client.list_resources()\n",
    "    prompts = await client.list_prompts()\n",
    "    \n",
    "    print(tools, \"\\n\", resources,\"\\n\", prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6eb543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='get_current_datetime', title=None, description=\"Get the current date and time as a formatted string.\\n\\nReturns:\\n    Current date and time in 'YYYY-MM-DD HH:MM:SS' format.\", inputSchema={'properties': {}, 'type': 'object'}, outputSchema={'properties': {'result': {'type': 'string'}}, 'required': ['result'], 'type': 'object', 'x-fastmcp-wrap-result': True}, icons=None, annotations=None, meta={'_fastmcp': {'tags': []}}),\n",
       " Tool(name='get_workday_status', title=None, description='Determine if today is a workday or weekend.\\n\\nReturns:\\n    \"Workday\" if today is Monday to Friday, \"Weekend\" if Saturday or Sunday.', inputSchema={'properties': {}, 'type': 'object'}, outputSchema={'properties': {'result': {'type': 'string'}}, 'required': ['result'], 'type': 'object', 'x-fastmcp-wrap-result': True}, icons=None, annotations=None, meta={'_fastmcp': {'tags': []}})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c7fc7",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28327a4",
   "metadata": {},
   "source": [
    "# From src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c465682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /Users/clemenscremer/source/LFMSystem/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m23 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m lfmsystem\u001b[2m @ file:///Users/clemenscremer/source/LFMSystem\u001b[0m   \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m lfmsystem\u001b[2m @ file:///Users/clemenscremer/source/LFMSystem\u001b[0m\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m lfmsystem\u001b[2m @ file:///Users/clemenscremer/source/LFMSystem\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 345ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.53ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0mrom file:///Users/clemenscremer/so\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mlfmsystem\u001b[0m\u001b[2m==0.1.0 (from file:///Users/clemenscremer/source/LFMSystem)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e206a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Executing Tool...\n",
      "The current weather in London is rainy with a temperature of 19 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# notebooks/playground.ipynb\n",
    "from lfmsystem.registry import ToolRegistry\n",
    "from lfmsystem.agent import LiquidAgent\n",
    "# Import your standard tools\n",
    "from lfmsystem.tools import get_current_time, get_weather, calculate_bmi\n",
    "\n",
    "# 1. Config\n",
    "MODEL_NAME = \"hf.co/LiquidAI/LFM2-1.2B-Tool-GGUF:Q4_K_M\"\n",
    "MODEL_SETTINGS = {\n",
    "    \"temperature\": 0.3,\n",
    "    \"min_p\": 0.15,\n",
    "    \"repetition_penalty\": 1.05\n",
    "}\n",
    "\n",
    "# 2. Create Registry and Pick your Tools\n",
    "registry = ToolRegistry()\n",
    "\n",
    "# You can register imported functions easily:\n",
    "registry.register(get_current_time)\n",
    "registry.register(get_weather)\n",
    "# maybe you don't want the agent to calculate BMI today, so you just don't register it!\n",
    "\n",
    "# 3. Launch Agent\n",
    "agent = LiquidAgent(MODEL_NAME, registry=registry, **MODEL_SETTINGS)\n",
    "\n",
    "print(agent.chat(\"What is the weather in London?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9588185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided text appears to be a continuation of a longer document, but it seems cut off. It discusses the \"Fairness and Transparency in Artificial Intelligence\" initiative, which aims to ensure that AI systems are fair, transparent, and accountable. The initiative involves various stakeholders, including researchers, policymakers, industry leaders, and civil society organizations. Key aspects covered include:\\n\\n1. **Ethical Guidelines**: Establishing ethical principles for AI development and deployment.\\n2. **Bias Mitigation**: Techniques to identify and reduce bias in AI algorithms.\\n3. **Explainability**: Making AI decision-making processes understandable to users.\\n4. **Accountability**: Defining responsibilities and mechanisms for addressing harm caused by AI systems.\\n5. **Regulatory Frameworks**: Developing laws and regulations to govern AI use.\\n6. **Public Engagement**: Involving the public in discussions about AI policies and impacts.\\n7. **Collaborative Research**: Encouraging interdisciplinary research to advance fairness and transparency in AI.\\n\\nThe document emphasizes the importance of a multi-stakeholder approach to address the complex challenges posed by AI, highlighting that collaboration is essential for creating trustworthy and beneficial AI systems.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lfmsystem.agent import SimpleBot\n",
    "# Just a simple chatter\n",
    "summarizer = SimpleBot(MODEL_NAME, system_prompt=\"Summarize text\", **MODEL_SETTINGS)\n",
    "summarizer.chat(\"Here is some long text...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfmsystem (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
